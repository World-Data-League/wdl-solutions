{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Data League 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Submission Template\n",
    "\n",
    "This notebook is one of the mandatory deliverables when you submit your solution. Its structure follows the WDL evaluation criteria and it has dedicated cells where you should add information. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work. Make sure to list all the datasets used besides the ones provided.\n",
    "\n",
    "Instructions:\n",
    "1. üß± Create a separate copy of this template and **do not change** the predefined structure\n",
    "2. üë• Fill in the Authors section with the name of each team member\n",
    "3. üíª Develop your code - make sure to add comments and save all the output you want the jury to see. Your code **must be** runnable!\n",
    "4. üìÑ Fill in all the text sections\n",
    "5. üóëÔ∏è Remove this section (‚ÄòNotebook Submission Template‚Äô) and any instructions inside other sections\n",
    "6. üì• Export as HTML and make sure all the visualisations are visible.\n",
    "7. ‚¨ÜÔ∏è Upload the .ipynb file to the submission platform and make sure that all the visualisations are visible and everything (text, images, ..) in all deliverables renders correctly.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Challenge\n",
    "Determining The Main Mobility Flows in the City of Lisbon Based on Mobile Device Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team: (Insert Team Name Here)\n",
    "## üë• Authors\n",
    "* Eduardo Silva\n",
    "* Pedro Lopes\n",
    "* Tom√°s Pereira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Development\n",
    "Start coding here! üê±‚Äçüèç\n",
    "\n",
    "Create the necessary subsections (e.g. EDA, different experiments, etc..) and markdown cells to include descriptions of your work where you see fit. Comment your code. \n",
    "\n",
    "All new subsections must start with three hash characters. More specifically, don't forget to explore the following:\n",
    "1. Assess the data quality\n",
    "2. Make sure you have a good EDA where you enlist all the insights\n",
    "3. Explain the process for feature engineering and cleaning\n",
    "4. Discuss the model / technique(s) selection\n",
    "5. Don't forget to explore model interpretability and fairness or justify why it is not needed\n",
    "\n",
    "Pro-tip 1: Don't forget to make the jury's life easier. Remove any unnecessary prints before submitting the work. Hide any long output cells (from training a model for example). For each subsection, have a quick introduction (justifying what you are about to do) and conclusion (results you got from what you did). \n",
    "\n",
    "Pro-tip 2: Have many similiar graphs which all tell the same story? Add them to the appendix and show only a couple of examples, with the mention that all the others are in the appendix.\n",
    "\n",
    "Pro-tip 3: Don't forget to have a motivate all of your choices, these can be: Data-driven, constraints-driven, literature-driven or a combination of any. For example, why did you choose to test certain algorithms or why only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import typing\n",
    "from geopy.distance import geodesic as GD \n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.metrics import DistanceMetric\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import folium\n",
    "import geopandas as gpd \n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "CRS = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "mobile_devices_sep_path = os.environ[\"CML_DISPOSITIVOS MOVEIS_GRELHA_indCD15m_2022_09_0001_4000\"] # DATASET 7 September\n",
    "mobile_devices_oct_path = os.environ[\"CML_DISPOSITIVOS MOVEIS_GRELHA_indCD15m_2022_10_0001_4000\"] # DATASET 7 OCTOBER\n",
    "mobile_devices_nov_path = os.environ[\"CML_DISPOSITIVOS MOVEIS_GRELHA_indCD15m_2022_11_0001_4000\"] # DATASET 7 November\n",
    "\n",
    "mobile_devices_proc_sep_path = os.environ[\"RESULTS_09\"]\n",
    "mobile_devices_proc_oct_path = os.environ[\"RESULTS_10\"]\n",
    "mobile_devices_proc_nov_path = os.environ[\"RESULTS_11\"]\n",
    "\n",
    "metro_station_path = os.environ[\"METRO_STATATION\"] # EXTERNAL DATASET\n",
    "\n",
    "grid_path = os.environ[\"DISPOSITIVOS MOVEIS_QUADRICULAS\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_devices_sept = pd.read_csv(mobile_devices_sep_path, nrows=100000) # MODIFY TO USE ALL THE DATA HERE\n",
    "mobile_devices_oct = pd.read_csv(mobile_devices_sep_path, nrows=100) # MODIFY TO USE ALL THE DATA HERE\n",
    "mobile_devices_nov = pd.read_csv(mobile_devices_sep_path, nrows=100) # MODIFY TO USE ALL THE DATA HERE\n",
    "\n",
    "mobile_devices = pd.concat([mobile_devices_sept, mobile_devices_oct, mobile_devices_nov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mobile_device_dataset(mobile_devices):\n",
    "    # Format columns\n",
    "    conversion_columns = {\n",
    "        \"C1\": \"n_terminals\", \n",
    "        \"C2\": \"n_terminals_roaming\",\n",
    "        \"C3\": \"terminals_remain\",\n",
    "        \"C4\": \"terminals_remain_roaming\",\n",
    "        \"C5\": \"in_flow\", \n",
    "        \"C6\": \"out_flow\",\n",
    "        \"C7\": \"in_flow_roaming\", \n",
    "        \"C8\": \"out_flow_roaming\"\n",
    "        }\n",
    "    mobile_devices = mobile_devices.rename(columns=conversion_columns)\n",
    "    mobile_devices[\"total_flow\"] = mobile_devices[\"in_flow\"] + mobile_devices[\"out_flow\"]\n",
    "\n",
    "    mobile_devices[\"Datetime\"] = pd.to_datetime(mobile_devices[\"Datetime\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    mobile_devices[\"weekday\"] = mobile_devices[\"Datetime\"].dt.dayofweek\n",
    "    mobile_devices[\"time_of_day\"] = mobile_devices[\"Datetime\"].dt.hour\n",
    "\n",
    "    return mobile_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_devices = prepare_mobile_device_dataset(mobile_devices)\n",
    "rush_hours_df = mobile_devices # UNCOMMENT FOLLOWING LINE TO ADD RUSH HOUR FILTER\n",
    "# rush_hours_df = mobile_devices[(mobile_devices[\"weekday\"].isin([0,1,2,3,4]) & mobile_devices[\"time_of_day\"].isin([7,8,9,10,17,18,19,20]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mobile_devices))\n",
    "print(len(rush_hours_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['n_terminals', 'n_terminals_roaming', 'terminals_remain', 'terminals_remain_roaming', 'in_flow', 'out_flow', 'in_flow_roaming', 'out_flow_roaming', 'total_flow']\n",
    "pivot = pd.pivot_table(\n",
    "    data=rush_hours_df,\n",
    "    index='time_of_day',\n",
    "    aggfunc='sum',\n",
    "    values=columns\n",
    ")\n",
    "\n",
    "for col in columns:\n",
    "    pivot[col].plot(kind='bar', width=1.0)\n",
    "    plt.savefig(f'{col}.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY AT INTERMEDIATE STEP DATA LOADING\n",
    "grid_df = pd.read_excel(grid_path)\n",
    "\n",
    "# INTERMEDIATE STEPS DATASET\n",
    "# mobile_devices_sept = pd.read_csv(mobile_devices_proc_sep_path)\n",
    "# mobile_devices_oct = pd.read_csv(mobile_devices_proc_oct_path)\n",
    "# mobile_devices_nov = pd.read_csv(mobile_devices_proc_nov_path)\n",
    "# mobile_devices = pd.concat([mobile_devices_sept, mobile_devices_oct, mobile_devices_nov])\n",
    "# mobile_devices = mobile_devices[(mobile_devices[\"weekday\"].isin([0,1,2,3,4]) & mobile_devices[\"time_of_day\"].isin([7,8,9,10,17,18,19,20]))]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRID AND TERMINALS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_id_matrix():\n",
    "    # Create a list of tuples\n",
    "    tuples = grid_df.apply(lambda row: (row[\"grelha_id\"], row[\"latitude\"], row[\"longitude\"]), axis=1).tolist()\n",
    "    # Sort the tuples by latitude and longitude\n",
    "    tuples.sort(key=lambda x: (x[1], x[2]))\n",
    "\n",
    "    latitude_tuples = {ele[1] for ele in tuples}\n",
    "    longitude_tuples = {ele[2] for ele in tuples}\n",
    "\n",
    "    # Define the boundaries of the matrix\n",
    "    lat_min, lat_max = max(grid_df[\"latitude\"]), min(grid_df[\"latitude\"])\n",
    "    lon_min, lon_max = max(grid_df[\"longitude\"]), min(grid_df[\"longitude\"])\n",
    "\n",
    "    # Calculate the size of each cell in latitude and longitude degrees\n",
    "    lat_size = (lat_max - lat_min) / len(latitude_tuples)\n",
    "    lon_size = (lon_max - lon_min) / len(longitude_tuples)\n",
    "\n",
    "    # Create a 3x3 matrix with NaN values\n",
    "    matrix = np.empty((len(latitude_tuples), len(longitude_tuples)))\n",
    "    matrix[:] = np.nan\n",
    "\n",
    "    # Populate the matrix with the sorted tuples\n",
    "    for i, t in enumerate(tuples):\n",
    "        grid_id, lat, lon = t\n",
    "        lat_idx = int((lat - lat_min) // lat_size) - 1\n",
    "        lon_idx = int((lon - lon_min) // lon_size) - 1\n",
    "        matrix[lat_idx, lon_idx] = i\n",
    "\n",
    "    grid_id_matrix = []\n",
    "    for arr in matrix:\n",
    "        grid_id_line = []\n",
    "\n",
    "        for i in arr:\n",
    "            if math.isnan(i):\n",
    "                grid_id_line.append(np.nan)\n",
    "            else:\n",
    "                grid_id_line.append(tuples[int(i)][0])\n",
    "\n",
    "        grid_id_matrix.append(grid_id_line)\n",
    "\n",
    "    return grid_id_matrix\n",
    "\n",
    "def get_adjacent_grid_ids(matrix, d1_pos, d2_pos):\n",
    "    # d1 is the first dimension, so vertical axis\n",
    "    # d1 is the second dimension, so horizontal axis\n",
    "    max_d1 = len(matrix) - 1\n",
    "    max_d2 = len(matrix[0]) - 1\n",
    "\n",
    "    assert d1_pos >= 0\n",
    "    assert d1_pos <= max_d1\n",
    "    assert d2_pos >= 0\n",
    "    assert d2_pos <= max_d2\n",
    "\n",
    "    adjacent_grid_id_list = []\n",
    "\n",
    "    up_d1_pos = d1_pos - 1 if (d1_pos - 1) >= 0 else -1\n",
    "    up_d2_pos = d2_pos\n",
    "\n",
    "    if up_d1_pos != -1 and not math.isnan(matrix[up_d1_pos][up_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[up_d1_pos][up_d2_pos])\n",
    "\n",
    "    down_d1_pos = d1_pos + 1 if (d1_pos + 1) <= max_d1 else -1\n",
    "    down_d2_pos = d2_pos\n",
    "\n",
    "    if down_d1_pos != -1 and not math.isnan(matrix[down_d1_pos][down_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[down_d1_pos][down_d2_pos])\n",
    "\n",
    "    left_d1_pos = d1_pos\n",
    "    left_d2_pos = d2_pos - 1 if (d2_pos - 1) >= 0 else -1\n",
    "\n",
    "    if left_d2_pos != -1 and not math.isnan(matrix[left_d1_pos][left_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[left_d1_pos][left_d2_pos])\n",
    "\n",
    "    right_d1_pos = d1_pos\n",
    "    right_d2_pos = d2_pos + 1 if (d2_pos + 1) <= max_d2 else -1\n",
    "\n",
    "    if right_d1_pos != -1 and not math.isnan(matrix[right_d1_pos][right_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[right_d1_pos][right_d2_pos])\n",
    "\n",
    "    up_left_d1_pos = d1_pos - 1 if (d1_pos - 1) >= 0 else -1\n",
    "    up_left_d2_pos = d2_pos - 1 if (d2_pos - 1) >= 0 else -1\n",
    "\n",
    "    if up_left_d1_pos != -1 and up_left_d1_pos != -1 and not math.isnan(matrix[up_left_d1_pos][up_left_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[up_left_d1_pos][up_left_d2_pos])\n",
    "\n",
    "    up_right_d1_pos = d1_pos - 1 if (d1_pos - 1) >= 0 else -1\n",
    "    up_right_d2_pos = d2_pos + 1 if (d2_pos + 1) <= max_d2 else -1\n",
    "\n",
    "    if up_right_d1_pos != -1 and up_right_d2_pos != -1 and not math.isnan(matrix[up_right_d1_pos][up_right_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[up_right_d1_pos][up_right_d2_pos])\n",
    "\n",
    "    down_left_d1_pos = d1_pos + 1 if (d1_pos + 1) <= max_d1 else -1\n",
    "    down_left_d2_pos = d2_pos - 1 if (d2_pos - 1) >= 0 else -1\n",
    "\n",
    "    if down_left_d1_pos != -1 and down_left_d2_pos != -1 and not math.isnan(matrix[down_left_d1_pos][down_left_d2_pos]):\n",
    "        adjacent_grid_id_list.append(matrix[down_left_d1_pos][down_left_d2_pos])\n",
    "\n",
    "    down_right_d1_pos = d1_pos + 1 if (d1_pos + 1) <= max_d1 else -1\n",
    "    down_right_d2_pos = d2_pos + 1 if (d2_pos + 1) <= max_d2 else -1\n",
    "\n",
    "    if (\n",
    "        down_right_d1_pos != -1\n",
    "        and down_right_d2_pos != -1\n",
    "        and not math.isnan(matrix[down_right_d1_pos][down_right_d2_pos])\n",
    "    ):\n",
    "        adjacent_grid_id_list.append(matrix[down_right_d1_pos][down_right_d2_pos])\n",
    "\n",
    "    return adjacent_grid_id_list\n",
    "\n",
    "\n",
    "def get_adjacent_grid_dict(grid_id_matrix):\n",
    "    adjacent_grid_dict = {}\n",
    "    for i in range(len(grid_id_matrix)):\n",
    "        for j in range(len(grid_id_matrix[0])):\n",
    "            if not math.isnan(grid_id_matrix[i][j]):\n",
    "                adjacent_grid_dict[grid_id_matrix[i][j]] = get_adjacent_grid_ids(grid_id_matrix, i, j)\n",
    "\n",
    "    return adjacent_grid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_data(group):\n",
    "    current_id = group[\"Grid_ID\"].iloc[0]\n",
    "\n",
    "    adjacent_square_list = adjacent_grid_dict[current_id]\n",
    "\n",
    "    adjacent_square_df = mobile_devices[mobile_devices[\"Grid_ID\"].isin(adjacent_square_list)]\n",
    "\n",
    "    data_columns = [\n",
    "        \"n_terminals\",\n",
    "        \"n_terminals_roaming\",\n",
    "        \"terminals_remain\",\n",
    "        \"terminals_remain_roaming\",\n",
    "        \"in_flow\",\n",
    "        \"out_flow\",\n",
    "        \"in_flow_roaming\",\n",
    "        \"out_flow_roaming\",\n",
    "        \"total_flow\",\n",
    "    ]\n",
    "    data_groups = adjacent_square_df.groupby(\"Datetime\")\n",
    "\n",
    "    aggregation = data_groups[data_columns].aggregate(\"sum\")\n",
    "    aggregation[\"Grid_ID\"] = current_id\n",
    "\n",
    "    merged_df = pd.merge(group, aggregation, on=[\"Grid_ID\", \"Datetime\"], suffixes=[\"\", \"_adjacent\"])\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create auxiliar info\n",
    "grid_id_matrix = create_grid_id_matrix()\n",
    "adjacent_grid_dict = get_adjacent_grid_dict(grid_id_matrix)\n",
    "\n",
    "# Get Aggregated data\n",
    "grouped_df = mobile_devices.groupby(\"Grid_ID\")\n",
    "\n",
    "result_df = grouped_df.apply(get_adjacent_data)\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metro DATASET MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_square_center_to_polygon(point: Point, square_length: int) -> Polygon:\n",
    "    \"\"\"Converts a grid square's center Point to a spacial Polygon.\n",
    "\n",
    "    Args:\n",
    "        point (Point): A square's center point (with longitude / latitude information).\n",
    "        square_length (int): Square's side length.\n",
    "\n",
    "    Returns:\n",
    "        Polygon: The polygon representing the grid square.\n",
    "    \"\"\"\n",
    "\n",
    "    half_length = square_length / 2\n",
    "\n",
    "    # Get center-aligned top and bottom points\n",
    "    center_north = GD(meters=half_length).destination((point.x, point.y), bearing=0)\n",
    "    center_south = GD(meters=half_length).destination((point.x, point.y), bearing=180)\n",
    "\n",
    "    # Get top corners\n",
    "    north_east = GD(meters=half_length).destination(center_north, 90)\n",
    "    north_west = GD(meters=half_length).destination(center_north, 270)\n",
    "\n",
    "    # Get bottom corners\n",
    "    south_east = GD(meters=half_length).destination(center_south, 90)\n",
    "    south_west = GD(meters=half_length).destination(center_south, 270)\n",
    "\n",
    "\n",
    "    return Polygon([north_east, south_east, south_west, north_west])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_squares_dataset(file_path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Parses the Lisbon grid squares dataset.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to file.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame containing the information of each grid square represented by spacial polygons.\n",
    "    \"\"\"\n",
    "    # read the CSV file with the square information\n",
    "    # squares_df = pd.read_csv(file_path)\n",
    "    squares_df = pd.read_excel(file_path)\n",
    "\n",
    "    # create a Point object for each square\"s location\n",
    "    squares_df[\"center_point\"] = [Point(xy) for xy in zip(squares_df[\"longitude\"], squares_df[\"latitude\"])]\n",
    "    squares_gdf = gpd.GeoDataFrame(squares_df, crs=CRS, geometry=\"center_point\")\n",
    "\n",
    "    squares_gdf[\"geometry\"] = squares_gdf[\"center_point\"].apply(lambda x: convert_square_center_to_polygon(x, 200))\n",
    "    squares_gdf.set_geometry(\"geometry\", inplace=True)\n",
    "    squares_gdf.drop(\"center_point\", axis=1, inplace=True)\n",
    "\n",
    "    return squares_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metro_stations_file(file_path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Parses JSON file containing metro stations information in the city of Lisbon.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame containing the information regarding the metron stations.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    features = data[\"features\"]\n",
    "    rows = []\n",
    "\n",
    "    for feature in features:\n",
    "        properties = feature[\"properties\"]\n",
    "        nome = properties[\"NOME\"]\n",
    "        coordinates = feature[\"geometry\"][\"coordinates\"]\n",
    "        longitude = coordinates[0]\n",
    "        latitude = coordinates[1]\n",
    "\n",
    "        rows.append({\"Nome\": nome, \"Longitude\": longitude, \"Latitude\": latitude, \"location\": Point(longitude, latitude)})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, crs=CRS, geometry=\"location\")\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_of_interest_squares(squares_gdf: gpd.GeoDataFrame, poi_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Returns the squares with at least one point of interest.\n",
    "\n",
    "    Args:\n",
    "        squares_gdf (gpd.GeoDataFrame): The grid squares GeoDataFrame.\n",
    "        poi_gdf (gpd.GeoDataFrame): The GeoDataFrame containing the location of each point of interest.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: The grid squares with at least one point of interest.\n",
    "    \"\"\"\n",
    "    within = gpd.sjoin(poi_gdf, squares_gdf, predicate=\"within\")\n",
    "    within_counts = within.groupby(\"index_right\").size()\n",
    "    within_counts.name = \"num_poi\"\n",
    "    within_counts = within_counts.reset_index()\n",
    "    within_counts.set_index(\"index_right\", inplace=True)\n",
    "\n",
    "    squares_poi_gdf = squares_gdf.merge(within_counts, left_index=True, right_index=True, how=\"left\")\n",
    "    squares_poi_gdf[\"num_poi\"] = squares_poi_gdf[\"num_poi\"].fillna(0)\n",
    "\n",
    "    has_poi = squares_poi_gdf[\"num_poi\"] > 0\n",
    "    \n",
    "    return squares_gdf[has_poi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_to_nearest_square_with_poi(grid_gdf: gpd.GeoDataFrame, grid_poi_gdf: gpd.GeoDataFrame, new_col_name: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Calculates, for each grid square, the distance (in squares) to the nearest square with a point of interest.\n",
    "\n",
    "    Args:\n",
    "        grid_gdf (gpd.GeoDataFrame): The GeoDataFrame with the grid squares center points longitude and latitude.\n",
    "        grid_poi_gdf (gpd.GeoDataFrame): The GeoDataFrame with the grid squares containing points of interest and their longitude and latitude.\n",
    "        new_col_name (str): The new column name.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Modified grid square's GeoDataFrame with a new column representing the distance to the nearest square with\n",
    "        a point of interest.\n",
    "    \"\"\"\n",
    "    rad_grid_gdf = grid_gdf.copy()\n",
    "    rad_points_gdf = grid_poi_gdf.copy()\n",
    "    \n",
    "    rad_grid_gdf[\"longitude\"] = np.radians(rad_grid_gdf[\"longitude\"])\n",
    "    rad_grid_gdf[\"latitude\"] = np.radians(rad_grid_gdf[\"latitude\"])\n",
    "\n",
    "    rad_points_gdf[\"longitude\"] = np.radians(rad_points_gdf[\"longitude\"])\n",
    "    rad_points_gdf[\"latitude\"] = np.radians(rad_points_gdf[\"latitude\"])\n",
    "\n",
    "    dist = DistanceMetric.get_metric('haversine')\n",
    "\n",
    "    distances = dist.pairwise(rad_grid_gdf[['latitude','longitude']].to_numpy(), rad_points_gdf[['latitude','longitude']].to_numpy()) * 6373\n",
    "    distances = np.ceil(distances / 0.2)\n",
    "\n",
    "    distances = np.min(distances, axis=1)\n",
    "\n",
    "    new_grid_df = grid_gdf.copy()\n",
    "    new_grid_df[new_col_name] = distances\n",
    "\n",
    "    return new_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Metro Dataset\n",
    "squares_gdf = parse_squares_dataset(grid_path)\n",
    "metro_gdf = parse_metro_stations_file(metro_station_path)\n",
    "metro_squares_gdf = get_point_of_interest_squares(squares_gdf, metro_gdf)\n",
    "metro_dist_gdf = calc_distance_to_nearest_square_with_poi(squares_gdf, metro_squares_gdf, \"dist_min_metro\")\n",
    "\n",
    "metro_dist_df = pd.DataFrame(metro_dist_gdf[[\"grelha_id\", \"dist_min_metro\"]])\n",
    "metro_dist_df.rename({\"grelhad_id\": \"Grid_ID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training Dataset\n",
    "complete_df = mobile_devices.join(metro_dist_df, on=\"Grid_ID\")\n",
    "\n",
    "complete_df = complete_df[[\n",
    "        \"n_terminals\",\n",
    "        \"n_terminals_roaming\",\n",
    "        \"terminals_remain\",\n",
    "        \"terminals_remain_roaming\",\n",
    "        \"in_flow\",\n",
    "        \"out_flow\",\n",
    "        \"in_flow_roaming\",\n",
    "        \"out_flow_roaming\",\n",
    "        \"total_flow\", \n",
    "        'weekday', \n",
    "        'time_of_day',\n",
    "        'dist_min_metro'\n",
    "    ]]\n",
    "\n",
    "label_column = 'total_flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Split\n",
    "x_columns = list(complete_df.columns)\n",
    "x_columns.remove(label_column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_df[x_columns],complete_df[label_column], train_size=0.8)\n",
    "\n",
    "train_data = pd.DataFrame(X_train, columns=x_columns)\n",
    "train_data[label_column] = y_train\n",
    "\n",
    "test_data = pd.DataFrame(X_test, columns=x_columns)\n",
    "test_data[label_column] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_columns = list(mobile_devices.columns)\n",
    "# x_columns.remove(\"time_of_day\")\n",
    "\n",
    "# print(x_columns)\n",
    "\n",
    "# x_train, _, y_train, _ =  train_test_split(\n",
    "#     list(mobile_devices[x_columns].values),\n",
    "#     list(mobile_devices[\"time_of_day\"].values),\n",
    "#     train_size=0.03,\n",
    "#     stratify=list(mobile_devices[\"time_of_day\"].values)\n",
    "# )\n",
    "\n",
    "# mobile_devices_sampled = pd.DataFrame(x_train, columns=x_columns)\n",
    "# mobile_devices_sampled[\"time_of_day\"] = y_train\n",
    "\n",
    "# mobile_devices_sampled = mobile_devices.head(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_data, label=label_column)\n",
    "test_data = lgb.Dataset(test_data, label=label_column)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "space = [\n",
    "    Real(0.001, 0.9, name='learning_rate'),\n",
    "    Integer(0, 100, name='max_depth'),\n",
    "    Integer(2, 100, name='num_leaves'),\n",
    "    Integer(10, 1000, name=\"n_estimators\")\n",
    "]\n",
    "\n",
    "expected_flow_mae = complete_df[label_column].mean()\n",
    "expected_flow_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def objective(params: typing.Any) -> float:\n",
    "    \"\"\"Optimization fuction for the hyper-parameter search.\n",
    "\n",
    "    Args:\n",
    "        params (typing.Any): The hyper-parameter values.\n",
    "\n",
    "    Returns:\n",
    "        float: The MAE.\n",
    "    \"\"\"\n",
    "    # Extract hyperparameters from params\n",
    "    learning_rate, max_depth, num_leaves, n_estimators = params\n",
    "\n",
    "    # Train a LightGBM model with the given hyperparameters\n",
    "    model = lgb.LGBMRegressor(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        num_leaves=num_leaves,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_absolute_error(y_test, y_pred)\n",
    "    return mse\n",
    "    \n",
    "# Run hyperparameter search\n",
    "result = gp_minimize(objective, space, n_calls=10, random_state=42)\n",
    "# Print best hyperparameters and corresponding MSE\n",
    "print(f\"Best hyperparameters: {result.x}\")\n",
    "print(f\"Corresponding MAE: {result.fun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Visualisations\n",
    "Copy here the most important visualizations (graphs, charts, maps, images, etc). You can refer to them in the Executive Summary.\n",
    "\n",
    "Technical note: If not all the visualisations are visible, you can still include them as an image or link - in this case please upload them to your own repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(grid_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_MARKERS = False\n",
    "DISPLAY_CIRCLES = True\n",
    "DISPLAY_SQUARES = False\n",
    "DISPLAY_ONLY_100 = False\n",
    "\n",
    "m = folium.Map(location=[38.7223, -9.1393], zoom_start=\"13\", tiles=\"cartodb positron\")\n",
    "\n",
    "for i, item in enumerate(data.values):\n",
    "    coord = (item[1], item[2])\n",
    "    c1, c2, backAzimuth = pyproj.Geod(ellps='WGS84').fwd(lons=coord[0],lats= coord[1], az=45, dist=math.sqrt(2*(100**2))) \n",
    "    c2_1, c2_2, backAzimuth = pyproj.Geod(ellps='WGS84').fwd(lons=coord[0],lats= coord[1], az=225, dist=math.sqrt(2*(100**2))) \n",
    "\n",
    "    # folium.Rectangle([coord, (c1, c2)]).add_to(m)\n",
    "    # folium.Rectangle([coord, (c1_1, c1_2)]).add_to(m)\n",
    "    # folium.Rectangle([coord, (c2_1, c2_2)]).add_to(m)\n",
    "    if DISPLAY_SQUARES: \n",
    "        folium.Rectangle([(c2_1, c2_2), (c1, c2)]).add_to(m)\n",
    "\n",
    "    if DISPLAY_CIRCLES:\n",
    "        folium.Circle(location=coord,radius=100, fill_color='red').add_to(m)\n",
    "    if DISPLAY_MARKERS:\n",
    "        folium.Marker(\n",
    "            location=[item[1], item[2]]\n",
    "        ).add_to(m)\n",
    "    if DISPLAY_ONLY_100:\n",
    "        if i >= 100:\n",
    "            break\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëì References\n",
    "List all of the external links (even if they are already linked above), such as external datasets, papers, blog posts, code repositories and any other materials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Dataset esta√ß√µes de metro](https://dados.cm-lisboa.pt/dataset/estacoes-de-metro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è≠Ô∏è Appendix\n",
    "Add here any code, images or text that you still find relevant, but that was too long to include in the main report. This section is optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
